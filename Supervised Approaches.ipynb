{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK paths: ['/home/trinhthanh2508/Documents/sentiment-analysis/nltk_data', '/home/trinhthanh2508/nltk_data', '/home/trinhthanh2508/Documents/sentiment-analysis/.venv/nltk_data', '/home/trinhthanh2508/Documents/sentiment-analysis/.venv/share/nltk_data', '/home/trinhthanh2508/Documents/sentiment-analysis/.venv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "['Quick', 'test', ':', 'tokenization', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK setup: ensure punkt + punkt_tab are available and path is correct\n",
    "\n",
    "import os, nltk, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Prefer venv-local nltk_data\n",
    "venv_dir = Path.cwd() / \".venv\" / \"nltk_data\"\n",
    "project_dir = Path.cwd() / \"nltk_data\"\n",
    "user_dir = Path.home() / \"nltk_data\"\n",
    "\n",
    "for p in [venv_dir, project_dir, user_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    if str(p) not in nltk.data.path:\n",
    "        nltk.data.path.insert(0, str(p))\n",
    "\n",
    "# Download required resources quietly; NLTK>=3.9 needs punkt_tab too\n",
    "for pkg in [\"punkt\", \"punkt_tab\"]:\n",
    "    try:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: failed to download {pkg}: {e}\")\n",
    "\n",
    "print(\"NLTK paths:\", nltk.data.path)\n",
    "# Tiny sanity check\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    print(word_tokenize(\"Quick test: tokenization works.\", preserve_line=True))\n",
    "except Exception as e:\n",
    "    print(\"Tokenize sanity check failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>place</th>\n",
       "      <th>near</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lemma_sentence</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>lemma_sentence(with POS)</th>\n",
       "      <th>sentiword_analysis</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>senti_textblob</th>\n",
       "      <th>senti_wordnet</th>\n",
       "      <th>senti_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>293175196</td>\n",
       "      <td>sjtafalla</td>\n",
       "      <td>UK Parliament: 2nd Covid Vaccine should be 21 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>parliament covid vaccine should days not weeks...</td>\n",
       "      <td>['parliament', 'covid', 'vaccine', 'should', '...</td>\n",
       "      <td>['parliament', 'covid', 'vaccine', 'should', '...</td>\n",
       "      <td>parliament covid vaccine should day not week c...</td>\n",
       "      <td>[('parliament', 'n'), ('covid', 'n'), ('vaccin...</td>\n",
       "      <td>parliament covid vaccine should day not week...</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>1591779799</td>\n",
       "      <td>ellieelif</td>\n",
       "      <td>First dose of vaccination üíâ 5/1/2021..to comba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>dose vaccination syringe combating covid</td>\n",
       "      <td>['dose', 'vaccination', 'syringe', 'combating'...</td>\n",
       "      <td>['dose', 'vaccination', 'syringe', 'combating'...</td>\n",
       "      <td>dose vaccination syringe combating covid</td>\n",
       "      <td>[('dose', 'a'), ('vaccination', 'n'), ('syring...</td>\n",
       "      <td>dose vaccination syringe combat covid</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>215143656</td>\n",
       "      <td>danananarama</td>\n",
       "      <td>Time to forget about #COVID, #Brexit and #Trum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>time forget covid brexit trump sleep listening...</td>\n",
       "      <td>['time', 'forget', 'covid', 'brexit', 'trump',...</td>\n",
       "      <td>['time', 'forget', 'covid', 'brexit', 'trump',...</td>\n",
       "      <td>time forget covid brexit trump sleep listening...</td>\n",
       "      <td>[('time', 'n'), ('forget', 'v'), ('covid', 'a'...</td>\n",
       "      <td>time forget covid brexit trump sleep listeni...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>336462129</td>\n",
       "      <td>veronica_foote_</td>\n",
       "      <td>@doctor_oxford Rachel you absolutely nailed it...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>rachel absolutely nailed tonight program conti...</td>\n",
       "      <td>['rachel', 'absolutely', 'nailed', 'tonight', ...</td>\n",
       "      <td>['rachel', 'absolutely', 'nailed', 'tonight', ...</td>\n",
       "      <td>rachel absolutely nailed tonight program conti...</td>\n",
       "      <td>[('rachel', 'n'), ('absolutely', 'r'), ('naile...</td>\n",
       "      <td>rachel absolutely nailed tonight program con...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>1063705581133934593</td>\n",
       "      <td>5herii</td>\n",
       "      <td>My kids can never say they don‚Äôt wanna do thei...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>kids never not wana homework sparkles covid sp...</td>\n",
       "      <td>['kids', 'never', 'not', 'wana', 'homework', '...</td>\n",
       "      <td>['kid', 'never', 'not', 'wana', 'homework', 's...</td>\n",
       "      <td>kid never not wana homework sparkle covid spar...</td>\n",
       "      <td>[('kids', 'n'), ('never', 'r'), ('not', 'r'), ...</td>\n",
       "      <td>kid never not wana homework sparkle covid sp...</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77316</th>\n",
       "      <td>77316</td>\n",
       "      <td>2021-05-20</td>\n",
       "      <td>1161700993840680961</td>\n",
       "      <td>PhilipCrook9</td>\n",
       "      <td>Very little interest by politicians and media ...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-1.476463, 50.883...</td>\n",
       "      <td>Southampton, England</td>\n",
       "      <td>very politicians media china economy thriving ...</td>\n",
       "      <td>['very', 'politicians', 'media', 'china', 'eco...</td>\n",
       "      <td>['very', 'politician', 'medium', 'china', 'eco...</td>\n",
       "      <td>very politician medium china economy thriving ...</td>\n",
       "      <td>[('very', 'r'), ('politicians', 'n'), ('media'...</td>\n",
       "      <td>very politician medium china economy thrive ...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.7485</td>\n",
       "      <td>-0.005556</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77317</th>\n",
       "      <td>77317</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>146596633</td>\n",
       "      <td>edwardjsault</td>\n",
       "      <td>#BREAKING Health Secretary Matt Hancock has an...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-1.476463, 50.883...</td>\n",
       "      <td>Southampton, England</td>\n",
       "      <td>breaking health secretary matt hancock announc...</td>\n",
       "      <td>['breaking', 'health', 'secretary', 'matt', 'h...</td>\n",
       "      <td>['breaking', 'health', 'secretary', 'matt', 'h...</td>\n",
       "      <td>breaking health secretary matt hancock announc...</td>\n",
       "      <td>[('breaking', 'v'), ('health', 'n'), ('secreta...</td>\n",
       "      <td>break health secretary matt hancock announce...</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77318</th>\n",
       "      <td>77318</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>235304684</td>\n",
       "      <td>phoTomics</td>\n",
       "      <td>‚ÄúHundreds of flights full of Indian corona‚Äù co...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-1.476463, 50.883...</td>\n",
       "      <td>Southampton, England</td>\n",
       "      <td>hundreds flights indian corona pretty racist</td>\n",
       "      <td>['hundreds', 'flights', 'indian', 'corona', 'p...</td>\n",
       "      <td>['hundred', 'flight', 'indian', 'corona', 'pre...</td>\n",
       "      <td>hundred flight indian corona pretty racist</td>\n",
       "      <td>[('hundreds', 'n'), ('flights', 'n'), ('indian...</td>\n",
       "      <td>hundred flight indian corona pretty racist</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.2023</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77319</th>\n",
       "      <td>77319</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>726443182997835778</td>\n",
       "      <td>SarahClift10</td>\n",
       "      <td>Sadiq Khan asks for vaccine blitz in variant-h...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-1.476463, 50.883...</td>\n",
       "      <td>Southampton, England</td>\n",
       "      <td>sadiq khan vaccine blitz variant hit london bo...</td>\n",
       "      <td>['sadiq', 'khan', 'vaccine', 'blitz', 'variant...</td>\n",
       "      <td>['sadiq', 'khan', 'vaccine', 'blitz', 'variant...</td>\n",
       "      <td>sadiq khan vaccine blitz variant hit london bo...</td>\n",
       "      <td>[('sadiq', 'n'), ('khan', 'n'), ('vaccine', 'n...</td>\n",
       "      <td>sadiq khan vaccine blitz variant hit london ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77320</th>\n",
       "      <td>77320</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>316385442</td>\n",
       "      <td>theveindoc</td>\n",
       "      <td>Is this not Covid reality rather than Brexit r...</td>\n",
       "      <td>{'type': 'Feature', 'bbox': [-1.476463, 50.883...</td>\n",
       "      <td>Southampton, England</td>\n",
       "      <td>not covid reality brexit reality</td>\n",
       "      <td>['not', 'covid', 'reality', 'brexit', 'reality']</td>\n",
       "      <td>['not', 'covid', 'reality', 'brexit', 'reality']</td>\n",
       "      <td>not covid reality brexit reality</td>\n",
       "      <td>[('not', 'r'), ('covid', 'a'), ('reality', 'n'...</td>\n",
       "      <td>not covid reality brexit reality</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77321 rows √ó 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  created_at              user_id         username  \\\n",
       "0               0  2021-01-06            293175196        sjtafalla   \n",
       "1               1  2021-01-06           1591779799        ellieelif   \n",
       "2               2  2021-01-06            215143656     danananarama   \n",
       "3               3  2021-01-06            336462129  veronica_foote_   \n",
       "4               4  2021-01-06  1063705581133934593           5herii   \n",
       "...           ...         ...                  ...              ...   \n",
       "77316       77316  2021-05-20  1161700993840680961     PhilipCrook9   \n",
       "77317       77317  2021-05-19            146596633     edwardjsault   \n",
       "77318       77318  2021-05-19            235304684        phoTomics   \n",
       "77319       77319  2021-05-17   726443182997835778     SarahClift10   \n",
       "77320       77320  2021-05-17            316385442       theveindoc   \n",
       "\n",
       "                                                   tweet  \\\n",
       "0      UK Parliament: 2nd Covid Vaccine should be 21 ...   \n",
       "1      First dose of vaccination üíâ 5/1/2021..to comba...   \n",
       "2      Time to forget about #COVID, #Brexit and #Trum...   \n",
       "3      @doctor_oxford Rachel you absolutely nailed it...   \n",
       "4      My kids can never say they don‚Äôt wanna do thei...   \n",
       "...                                                  ...   \n",
       "77316  Very little interest by politicians and media ...   \n",
       "77317  #BREAKING Health Secretary Matt Hancock has an...   \n",
       "77318  ‚ÄúHundreds of flights full of Indian corona‚Äù co...   \n",
       "77319  Sadiq Khan asks for vaccine blitz in variant-h...   \n",
       "77320  Is this not Covid reality rather than Brexit r...   \n",
       "\n",
       "                                                   place  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "77316  {'type': 'Feature', 'bbox': [-1.476463, 50.883...   \n",
       "77317  {'type': 'Feature', 'bbox': [-1.476463, 50.883...   \n",
       "77318  {'type': 'Feature', 'bbox': [-1.476463, 50.883...   \n",
       "77319  {'type': 'Feature', 'bbox': [-1.476463, 50.883...   \n",
       "77320  {'type': 'Feature', 'bbox': [-1.476463, 50.883...   \n",
       "\n",
       "                       near  \\\n",
       "0                    London   \n",
       "1                    London   \n",
       "2                    London   \n",
       "3                    London   \n",
       "4                    London   \n",
       "...                     ...   \n",
       "77316  Southampton, England   \n",
       "77317  Southampton, England   \n",
       "77318  Southampton, England   \n",
       "77319  Southampton, England   \n",
       "77320  Southampton, England   \n",
       "\n",
       "                                             clean_tweet  \\\n",
       "0      parliament covid vaccine should days not weeks...   \n",
       "1               dose vaccination syringe combating covid   \n",
       "2      time forget covid brexit trump sleep listening...   \n",
       "3      rachel absolutely nailed tonight program conti...   \n",
       "4      kids never not wana homework sparkles covid sp...   \n",
       "...                                                  ...   \n",
       "77316  very politicians media china economy thriving ...   \n",
       "77317  breaking health secretary matt hancock announc...   \n",
       "77318       hundreds flights indian corona pretty racist   \n",
       "77319  sadiq khan vaccine blitz variant hit london bo...   \n",
       "77320                   not covid reality brexit reality   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      ['parliament', 'covid', 'vaccine', 'should', '...   \n",
       "1      ['dose', 'vaccination', 'syringe', 'combating'...   \n",
       "2      ['time', 'forget', 'covid', 'brexit', 'trump',...   \n",
       "3      ['rachel', 'absolutely', 'nailed', 'tonight', ...   \n",
       "4      ['kids', 'never', 'not', 'wana', 'homework', '...   \n",
       "...                                                  ...   \n",
       "77316  ['very', 'politicians', 'media', 'china', 'eco...   \n",
       "77317  ['breaking', 'health', 'secretary', 'matt', 'h...   \n",
       "77318  ['hundreds', 'flights', 'indian', 'corona', 'p...   \n",
       "77319  ['sadiq', 'khan', 'vaccine', 'blitz', 'variant...   \n",
       "77320   ['not', 'covid', 'reality', 'brexit', 'reality']   \n",
       "\n",
       "                                                   lemma  \\\n",
       "0      ['parliament', 'covid', 'vaccine', 'should', '...   \n",
       "1      ['dose', 'vaccination', 'syringe', 'combating'...   \n",
       "2      ['time', 'forget', 'covid', 'brexit', 'trump',...   \n",
       "3      ['rachel', 'absolutely', 'nailed', 'tonight', ...   \n",
       "4      ['kid', 'never', 'not', 'wana', 'homework', 's...   \n",
       "...                                                  ...   \n",
       "77316  ['very', 'politician', 'medium', 'china', 'eco...   \n",
       "77317  ['breaking', 'health', 'secretary', 'matt', 'h...   \n",
       "77318  ['hundred', 'flight', 'indian', 'corona', 'pre...   \n",
       "77319  ['sadiq', 'khan', 'vaccine', 'blitz', 'variant...   \n",
       "77320   ['not', 'covid', 'reality', 'brexit', 'reality']   \n",
       "\n",
       "                                          lemma_sentence  \\\n",
       "0      parliament covid vaccine should day not week c...   \n",
       "1               dose vaccination syringe combating covid   \n",
       "2      time forget covid brexit trump sleep listening...   \n",
       "3      rachel absolutely nailed tonight program conti...   \n",
       "4      kid never not wana homework sparkle covid spar...   \n",
       "...                                                  ...   \n",
       "77316  very politician medium china economy thriving ...   \n",
       "77317  breaking health secretary matt hancock announc...   \n",
       "77318         hundred flight indian corona pretty racist   \n",
       "77319  sadiq khan vaccine blitz variant hit london bo...   \n",
       "77320                   not covid reality brexit reality   \n",
       "\n",
       "                                                 pos_tag  \\\n",
       "0      [('parliament', 'n'), ('covid', 'n'), ('vaccin...   \n",
       "1      [('dose', 'a'), ('vaccination', 'n'), ('syring...   \n",
       "2      [('time', 'n'), ('forget', 'v'), ('covid', 'a'...   \n",
       "3      [('rachel', 'n'), ('absolutely', 'r'), ('naile...   \n",
       "4      [('kids', 'n'), ('never', 'r'), ('not', 'r'), ...   \n",
       "...                                                  ...   \n",
       "77316  [('very', 'r'), ('politicians', 'n'), ('media'...   \n",
       "77317  [('breaking', 'v'), ('health', 'n'), ('secreta...   \n",
       "77318  [('hundreds', 'n'), ('flights', 'n'), ('indian...   \n",
       "77319  [('sadiq', 'n'), ('khan', 'n'), ('vaccine', 'n...   \n",
       "77320  [('not', 'r'), ('covid', 'a'), ('reality', 'n'...   \n",
       "\n",
       "                                lemma_sentence(with POS)  sentiword_analysis  \\\n",
       "0        parliament covid vaccine should day not week...              -0.625   \n",
       "1                  dose vaccination syringe combat covid              -0.125   \n",
       "2        time forget covid brexit trump sleep listeni...               0.250   \n",
       "3        rachel absolutely nailed tonight program con...               0.625   \n",
       "4        kid never not wana homework sparkle covid sp...              -0.875   \n",
       "...                                                  ...                 ...   \n",
       "77316    very politician medium china economy thrive ...               0.250   \n",
       "77317    break health secretary matt hancock announce...               0.750   \n",
       "77318         hundred flight indian corona pretty racist              -0.125   \n",
       "77319    sadiq khan vaccine blitz variant hit london ...               0.000   \n",
       "77320                   not covid reality brexit reality              -0.625   \n",
       "\n",
       "       vader_score  textblob_polarity  senti_textblob  senti_wordnet  \\\n",
       "0          -0.0572           0.000000               0             -1   \n",
       "1          -0.3400           0.000000               0             -1   \n",
       "2          -0.2263           0.000000               0              1   \n",
       "3           0.0000           0.300000               1              1   \n",
       "4           0.0634           0.025000               1             -1   \n",
       "...            ...                ...             ...            ...   \n",
       "77316      -0.7485          -0.005556              -1              1   \n",
       "77317       0.0000           0.000000               0              1   \n",
       "77318      -0.2023           0.250000               1             -1   \n",
       "77319       0.0000           0.000000               0              0   \n",
       "77320       0.0000           0.000000               0             -1   \n",
       "\n",
       "       senti_vader  \n",
       "0               -1  \n",
       "1               -1  \n",
       "2               -1  \n",
       "3                0  \n",
       "4                1  \n",
       "...            ...  \n",
       "77316           -1  \n",
       "77317            0  \n",
       "77318           -1  \n",
       "77319            0  \n",
       "77320            0  \n",
       "\n",
       "[77321 rows x 19 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('supervised_sample_datasets/lexicon_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data\n",
    "positive_df = df[df[\"senti_vader\"] == 1]\n",
    "positive_df = positive_df[:15000] #15000 positive sentiment\n",
    "neutral_df = df[df[\"senti_vader\"] == 0]\n",
    "neutral_df = neutral_df[:15000] #15000 neutral sentiment\n",
    "negative_df = df[df[\"senti_vader\"] == -1]\n",
    "negative_df = negative_df[:15000] #15000 negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[positive_df, neutral_df, negative_df]\n",
    "df=pd.concat(df)\n",
    "df=df.reset_index(drop=True)\n",
    "df.to_csv('supervised_sample_datasets/sample_data.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after manual check\n",
    "df = pd.read_csv('supervised_sample_datasets/sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: 15000\n",
      "neutral 15000\n",
      "positive 15000\n"
     ]
    }
   ],
   "source": [
    "#after manual check\n",
    "negative_num=len(df[df['senti_vader'] < 0])\n",
    "print(\"negative:\", negative_num)\n",
    "neutral_num=len(df[df['senti_vader'] == 0])\n",
    "print(\"neutral\", neutral_num)\n",
    "positive_num=len(df[df['senti_vader'] > 0])\n",
    "print(\"positive\", positive_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found label columns: ['senti_vader', 'senti_textblob', 'senti_wordnet']\n",
      "Saved cleaned file: supervised_sample_datasets/lexicon_all_clean.csv\n",
      "Saved cleaned file: supervised_sample_datasets/lexicon_all_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lemma_sentence</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>lemma_sentence(with POS)</th>\n",
       "      <th>sentiword_analysis</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>senti_textblob</th>\n",
       "      <th>senti_wordnet</th>\n",
       "      <th>senti_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kids never not wana homework sparkles covid sp...</td>\n",
       "      <td>['kids', 'never', 'not', 'wana', 'homework', '...</td>\n",
       "      <td>['kid', 'never', 'not', 'wana', 'homework', 's...</td>\n",
       "      <td>kid never not wana homework sparkle covid spar...</td>\n",
       "      <td>[('kids', 'n'), ('never', 'r'), ('not', 'r'), ...</td>\n",
       "      <td>kid never not wana homework sparkle covid sp...</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not messing covid wear mask time london united...</td>\n",
       "      <td>['not', 'messing', 'covid', 'wear', 'mask', 't...</td>\n",
       "      <td>['not', 'messing', 'covid', 'wear', 'mask', 't...</td>\n",
       "      <td>not messing covid wear mask time london united...</td>\n",
       "      <td>[('not', 'r'), ('messing', 'v'), ('covid', 'n'...</td>\n",
       "      <td>not mess covid wear mask time london united ...</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0.6007</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>airfields cost remaining verses potential inco...</td>\n",
       "      <td>['airfields', 'cost', 'remaining', 'verses', '...</td>\n",
       "      <td>['airfield', 'cost', 'remaining', 'verse', 'po...</td>\n",
       "      <td>airfield cost remaining verse potential income...</td>\n",
       "      <td>[('airfields', 'n'), ('cost', 'n'), ('remainin...</td>\n",
       "      <td>airfield cost remain verse potential income ...</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remain astonished stock market not riots incre...</td>\n",
       "      <td>['remain', 'astonished', 'stock', 'market', 'n...</td>\n",
       "      <td>['remain', 'astonished', 'stock', 'market', 'n...</td>\n",
       "      <td>remain astonished stock market not riot incred...</td>\n",
       "      <td>[('remain', 'n'), ('astonished', 'a'), ('stock...</td>\n",
       "      <td>remain astonished stock market not riot incr...</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>0.6730</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lord jesus christ grace always listen prayers ...</td>\n",
       "      <td>['lord', 'jesus', 'christ', 'grace', 'always',...</td>\n",
       "      <td>['lord', 'jesus', 'christ', 'grace', 'always',...</td>\n",
       "      <td>lord jesus christ grace always listen prayer l...</td>\n",
       "      <td>[('lord', 'n'), ('jesus', 'n'), ('christ', 'n'...</td>\n",
       "      <td>lord jesus christ grace always listen prayer...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_tweet  \\\n",
       "0  kids never not wana homework sparkles covid sp...   \n",
       "1  not messing covid wear mask time london united...   \n",
       "2  airfields cost remaining verses potential inco...   \n",
       "3  remain astonished stock market not riots incre...   \n",
       "4  lord jesus christ grace always listen prayers ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['kids', 'never', 'not', 'wana', 'homework', '...   \n",
       "1  ['not', 'messing', 'covid', 'wear', 'mask', 't...   \n",
       "2  ['airfields', 'cost', 'remaining', 'verses', '...   \n",
       "3  ['remain', 'astonished', 'stock', 'market', 'n...   \n",
       "4  ['lord', 'jesus', 'christ', 'grace', 'always',...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  ['kid', 'never', 'not', 'wana', 'homework', 's...   \n",
       "1  ['not', 'messing', 'covid', 'wear', 'mask', 't...   \n",
       "2  ['airfield', 'cost', 'remaining', 'verse', 'po...   \n",
       "3  ['remain', 'astonished', 'stock', 'market', 'n...   \n",
       "4  ['lord', 'jesus', 'christ', 'grace', 'always',...   \n",
       "\n",
       "                                      lemma_sentence  \\\n",
       "0  kid never not wana homework sparkle covid spar...   \n",
       "1  not messing covid wear mask time london united...   \n",
       "2  airfield cost remaining verse potential income...   \n",
       "3  remain astonished stock market not riot incred...   \n",
       "4  lord jesus christ grace always listen prayer l...   \n",
       "\n",
       "                                             pos_tag  \\\n",
       "0  [('kids', 'n'), ('never', 'r'), ('not', 'r'), ...   \n",
       "1  [('not', 'r'), ('messing', 'v'), ('covid', 'n'...   \n",
       "2  [('airfields', 'n'), ('cost', 'n'), ('remainin...   \n",
       "3  [('remain', 'n'), ('astonished', 'a'), ('stock...   \n",
       "4  [('lord', 'n'), ('jesus', 'n'), ('christ', 'n'...   \n",
       "\n",
       "                            lemma_sentence(with POS)  sentiword_analysis  \\\n",
       "0    kid never not wana homework sparkle covid sp...              -0.875   \n",
       "1    not mess covid wear mask time london united ...              -0.625   \n",
       "2    airfield cost remain verse potential income ...              -0.500   \n",
       "3    remain astonished stock market not riot incr...              -1.250   \n",
       "4    lord jesus christ grace always listen prayer...               0.500   \n",
       "\n",
       "   vader_score  textblob_polarity  senti_textblob  senti_wordnet  senti_vader  \n",
       "0       0.0634             0.0250               1             -1            1  \n",
       "1       0.6007             0.0875               1             -1            1  \n",
       "2       0.5267             0.0000               0             -1            1  \n",
       "3       0.6730             0.5000               1             -1            1  \n",
       "4       0.7717             0.0000               0              1            1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean dataset and save a compact CSV for modeling\n",
    "cols_to_drop = ['Unnamed: 0','created_at','user_id','username','place','near','tweet']\n",
    "for c in cols_to_drop:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(c, axis=1)\n",
    "\n",
    "# Check for label columns\n",
    "label_candidates = [col for col in ['senti_vader','senti_textblob','senti_wordnet'] if col in df.columns]\n",
    "if not label_candidates:\n",
    "    print('Warning: no label columns found in DataFrame. Check source file before training')\n",
    "else:\n",
    "    print('Found label columns:', label_candidates)\n",
    "\n",
    "# Save cleaned CSV (no index)\n",
    "clean_path = 'supervised_sample_datasets/lexicon_all_clean.csv'\n",
    "df.to_csv(clean_path, index=False, encoding='utf_8_sig')\n",
    "print(f'Saved cleaned file: {clean_path}')\n",
    "\n",
    "# Preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37002, 29242)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BoW\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def bag_of_words(df):\n",
    "#     bow_vectorizer = CountVectorizer(max_df=0.90, min_df=0.2, stop_words=None, tokenizer=word_tokenize) \n",
    "    bow_vectorizer = CountVectorizer() \n",
    "    bow = bow_vectorizer.fit_transform(df['lemma_sentence(with POS)']) \n",
    "    #print(bow_vectorizer.get_feature_names())\n",
    "    #print(bow_vectorizer.vocabulary_)\n",
    "    return bow\n",
    "\n",
    "df_bow=bag_of_words(df)\n",
    "df_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37002, 29242)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "def tf_idf(df):\n",
    "#     tf_idf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.2, stop_words=None, tokenizer=word_tokenize, norm='l2') \n",
    "    tf_idf_vectorizer = TfidfVectorizer(norm='l2') #extract features\n",
    "    tfidf = tf_idf_vectorizer.fit_transform(df['lemma_sentence(with POS)']) #vectors\n",
    "    return tfidf\n",
    "df_tfidf=tf_idf(df)\n",
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [rachel, absolutely, nail, tonight, throughout...\n",
      "1    [kid, never, say, not, wana, homework, get, sp...\n",
      "2       [not, mess, wear, mask, time, united, kingdom]\n",
      "3    [problem, think, airfield, cost, remain, open,...\n",
      "4    [remain, astonished, stock, market, not, much,...\n",
      "Name: lemma_sentence(with POS), dtype: object\n",
      "Trained Word2Vec, vector_size= 200\n"
     ]
    }
   ],
   "source": [
    "#Word2vec\n",
    "\n",
    "#referenceÔºöhttps://www.pythonf.cn/read/93491\n",
    "\n",
    "#https://github.com/Shwetago/Sentiment_Analysis/blob/master/Twitter_Sentiment_Analysis.ipynb\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 2Ô∏è‚É£ Ch·∫Øc ch·∫Øn c·ªôt text l√† string\n",
    "texts = df['lemma_sentence(with POS)'].astype(str)\n",
    "\n",
    "# 3Ô∏è‚É£ Tokenize t·ª´ng c√¢u\n",
    "Tokenize_tweet = texts.apply(word_tokenize)\n",
    "print(Tokenize_tweet.head())\n",
    "\n",
    "# 4Ô∏è‚É£ Hu·∫•n luy·ªán Word2Vec (gensim 4+ uses vector_size)\n",
    "Model_W2V = gensim.models.Word2Vec(\n",
    "    sentences=Tokenize_tweet,   # danh s√°ch token\n",
    "    vector_size=200,            # thay size b·∫±ng vector_size\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,                       # skip-gram\n",
    "    hs=0,\n",
    "    negative=10,\n",
    "    workers=2,\n",
    "    seed=34\n",
    ")\n",
    "print(\"Trained Word2Vec, vector_size=\", getattr(Model_W2V, \"vector_size\", 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.168387</td>\n",
       "      <td>-0.125911</td>\n",
       "      <td>-0.053192</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>-0.184924</td>\n",
       "      <td>-0.048665</td>\n",
       "      <td>-0.039249</td>\n",
       "      <td>-0.187336</td>\n",
       "      <td>0.161375</td>\n",
       "      <td>-0.171687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006870</td>\n",
       "      <td>-0.056751</td>\n",
       "      <td>0.160192</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>0.136783</td>\n",
       "      <td>0.043862</td>\n",
       "      <td>-0.117840</td>\n",
       "      <td>-0.222804</td>\n",
       "      <td>0.200862</td>\n",
       "      <td>0.117491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.164014</td>\n",
       "      <td>-0.093200</td>\n",
       "      <td>-0.029003</td>\n",
       "      <td>0.050677</td>\n",
       "      <td>-0.228341</td>\n",
       "      <td>-0.060853</td>\n",
       "      <td>-0.043682</td>\n",
       "      <td>-0.109162</td>\n",
       "      <td>0.223059</td>\n",
       "      <td>-0.054414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070183</td>\n",
       "      <td>-0.112682</td>\n",
       "      <td>0.117482</td>\n",
       "      <td>0.015329</td>\n",
       "      <td>0.174871</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.034252</td>\n",
       "      <td>-0.134489</td>\n",
       "      <td>0.127557</td>\n",
       "      <td>0.072811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.171861</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>-0.116919</td>\n",
       "      <td>0.134451</td>\n",
       "      <td>-0.157908</td>\n",
       "      <td>-0.114593</td>\n",
       "      <td>0.124050</td>\n",
       "      <td>-0.172095</td>\n",
       "      <td>0.199021</td>\n",
       "      <td>-0.034861</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111159</td>\n",
       "      <td>-0.132372</td>\n",
       "      <td>0.051475</td>\n",
       "      <td>0.219525</td>\n",
       "      <td>0.174639</td>\n",
       "      <td>-0.071572</td>\n",
       "      <td>-0.178896</td>\n",
       "      <td>-0.024752</td>\n",
       "      <td>0.419446</td>\n",
       "      <td>0.138474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.079731</td>\n",
       "      <td>-0.069034</td>\n",
       "      <td>-0.020413</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>-0.153459</td>\n",
       "      <td>-0.002771</td>\n",
       "      <td>-0.020321</td>\n",
       "      <td>-0.143229</td>\n",
       "      <td>0.201097</td>\n",
       "      <td>-0.130508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>-0.029193</td>\n",
       "      <td>0.180018</td>\n",
       "      <td>-0.052088</td>\n",
       "      <td>0.297744</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>-0.147360</td>\n",
       "      <td>-0.142549</td>\n",
       "      <td>0.273239</td>\n",
       "      <td>0.076791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.110032</td>\n",
       "      <td>-0.092075</td>\n",
       "      <td>0.027431</td>\n",
       "      <td>0.055631</td>\n",
       "      <td>-0.196933</td>\n",
       "      <td>-0.008671</td>\n",
       "      <td>-0.064750</td>\n",
       "      <td>-0.179416</td>\n",
       "      <td>0.166853</td>\n",
       "      <td>-0.089843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061686</td>\n",
       "      <td>-0.092855</td>\n",
       "      <td>0.122646</td>\n",
       "      <td>-0.067085</td>\n",
       "      <td>0.227069</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>-0.138420</td>\n",
       "      <td>-0.103248</td>\n",
       "      <td>0.163166</td>\n",
       "      <td>0.027664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.168387 -0.125911 -0.053192  0.010157 -0.184924 -0.048665 -0.039249   \n",
       "1 -0.164014 -0.093200 -0.029003  0.050677 -0.228341 -0.060853 -0.043682   \n",
       "2 -0.171861 -0.000493 -0.116919  0.134451 -0.157908 -0.114593  0.124050   \n",
       "3 -0.079731 -0.069034 -0.020413 -0.001214 -0.153459 -0.002771 -0.020321   \n",
       "4 -0.110032 -0.092075  0.027431  0.055631 -0.196933 -0.008671 -0.064750   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0 -0.187336  0.161375 -0.171687  ... -0.006870 -0.056751  0.160192 -0.001608   \n",
       "1 -0.109162  0.223059 -0.054414  ... -0.070183 -0.112682  0.117482  0.015329   \n",
       "2 -0.172095  0.199021 -0.034861  ... -0.111159 -0.132372  0.051475  0.219525   \n",
       "3 -0.143229  0.201097 -0.130508  ...  0.039634 -0.029193  0.180018 -0.052088   \n",
       "4 -0.179416  0.166853 -0.089843  ... -0.061686 -0.092855  0.122646 -0.067085   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0  0.136783  0.043862 -0.117840 -0.222804  0.200862  0.117491  \n",
       "1  0.174871  0.028537 -0.034252 -0.134489  0.127557  0.072811  \n",
       "2  0.174639 -0.071572 -0.178896 -0.024752  0.419446  0.138474  \n",
       "3  0.297744  0.001276 -0.147360 -0.142549  0.273239  0.076791  \n",
       "4  0.227069  0.039300 -0.138420 -0.103248  0.163166  0.027664  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def word2vec_tweet(tokens, size=200):\n",
    "    vector = np.zeros(size)\n",
    "    vector_cnt = 0\n",
    "    for word in tokens:\n",
    "        if word in Model_W2V.wv:  # ki·ªÉm tra t·ª´ c√≥ trong vocab\n",
    "            vector += Model_W2V.wv[word]\n",
    "            vector_cnt += 1\n",
    "    if vector_cnt > 0:\n",
    "        vector /= vector_cnt  # average\n",
    "    return vector\n",
    "\n",
    "def word2vec_tweet_2(tokens, size=200):\n",
    "    vector = np.zeros(size)\n",
    "    for word in tokens:\n",
    "        if word in Model_W2V.wv:\n",
    "            vector += Model_W2V.wv[word]\n",
    "    return vector  # sum\n",
    "\n",
    "# t·∫°o ma tr·∫≠n tweet\n",
    "tweet_arr = np.zeros((len(Tokenize_tweet), 200))\n",
    "for i in range(len(Tokenize_tweet)):\n",
    "    tweet_arr[i, :] = word2vec_tweet(Tokenize_tweet[i], 200)\n",
    "\n",
    "tweet_vec_df = pd.DataFrame(tweet_arr)\n",
    "tweet_vec_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW for three classification models\n",
    "#split the train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_bow, df['senti_textblob'],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 20, 30, 40], 'min_samples_split': [2, 5, 10, 15], 'min_samples_leaf': [1, 2, 5, 10]}\n"
     ]
    }
   ],
   "source": [
    "#parameters in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] #tree number\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "max_depth = [10,20,30,40]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "# Create the param grid\n",
    "param_grid_forest = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "print(param_grid_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0], 'fit_prior': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "#parameters in MNB\n",
    "param_grid_nb = {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "                'fit_prior':[True, False]}\n",
    "print(param_grid_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [1, 10, 100, 1000], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': [1, 2, 3, 4]}\n"
     ]
    }
   ],
   "source": [
    "#parameters in SVC\n",
    "# c_list=list(range(1,51))\n",
    "param_grid_svc = {'C': [1, 10, 100, 1000],\n",
    "                  'kernel': ['linear','poly','rbf','sigmoid'],\n",
    "                  'degree': [1,2,3,4]}\n",
    "print(param_grid_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=5, min_samples_split=10, n_estimators=180; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=130; total time=   0.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=110; total time=   2.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=40; total time=   1.8s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.5s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=20; total time=   0.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=70; total time=   1.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   0.9s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   0.9s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   0.9s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=30; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.1s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.3s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.1s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.1s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.1s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=130; total time=   3.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=20; total time=   0.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.8s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.8s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=130; total time=   3.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 40,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 20}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for RF(with BoW)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RF_RandomGrid = RandomizedSearchCV(estimator = model_forest, param_distributions = param_grid_forest, cv = 10, verbose=2, n_jobs = 4)\n",
    "RF_RandomGrid.fit(x_train, y_train)\n",
    "RF_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': True, 'alpha': 0.5}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for MNB(with BoW)\n",
    "NB_RandomGrid = RandomizedSearchCV(estimator = model_nb, param_distributions = param_grid_nb, cv = 10, verbose=2, n_jobs = 4)\n",
    "NB_RandomGrid.fit(x_train, y_train)\n",
    "NB_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.5min\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.6min\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.8min\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 3.1min\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.7min\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 3.0min\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.7min\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.8min\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  40.5s\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  39.4s\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  41.1s\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  39.7s\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  41.9s\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.5min\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  37.1s\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  37.1s\n",
      "[CV] END .......................C=1, degree=3, kernel=linear; total time= 2.9min\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  42.3s\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  40.8s\n",
      "[CV] END ...................C=1000, degree=2, kernel=sigmoid; total time=  38.2s\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 7.3min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 7.7min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 7.7min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 8.2min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 7.8min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 8.3min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 7.6min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 8.0min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 5.6min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 5.6min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 7.2min\n",
      "[CV] END ........................C=10, degree=3, kernel=poly; total time= 7.0min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 5.0min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 4.8min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 4.8min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 4.9min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 4.9min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 4.7min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 4.9min\n",
      "[CV] END ........................C=100, degree=1, kernel=rbf; total time= 4.9min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.4min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.6min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.7min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.9min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.6min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.7min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.6min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.8min\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  34.7s\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  34.3s\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  34.6s\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  35.3s\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  35.8s\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  32.2s\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.6min\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  30.4s\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.7min\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  34.7s\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  34.1s\n",
      "[CV] END ...................C=1000, degree=1, kernel=sigmoid; total time=  32.3s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  59.3s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  55.5s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  58.3s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  56.0s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  58.0s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  57.8s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  55.3s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  56.4s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  35.9s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  35.8s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  55.7s\n",
      "[CV] END ......................C=1, degree=1, kernel=sigmoid; total time=  52.8s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  35.0s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  36.7s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  31.4s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  36.6s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  30.1s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  36.4s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  32.5s\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time=  34.8s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  34.0s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  36.6s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  36.8s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  38.2s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  37.9s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  34.9s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  35.8s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  37.4s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  39.1s\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time=  40.9s\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.1min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.3min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.4min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.4min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.5min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.4min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.5min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.4min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 2.8min\n",
      "[CV] END ....................C=1000, degree=4, kernel=linear; total time= 3.0min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'degree': 3, 'C': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for SVC(with BoW)\n",
    "SVC_RandomGrid = RandomizedSearchCV(estimator = model_svc, param_distributions = param_grid_svc, cv = 10, verbose=2, n_jobs = 4)\n",
    "SVC_RandomGrid.fit(x_train, y_train)\n",
    "SVC_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.13      0.24      1862\n",
      "           0       0.83      0.73      0.77      2367\n",
      "           1       0.60      0.96      0.74      3172\n",
      "\n",
      "    accuracy                           0.68      7401\n",
      "   macro avg       0.80      0.61      0.58      7401\n",
      "weighted avg       0.76      0.68      0.62      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model establishment and results(BoW)\n",
    "#Random Forest\n",
    "model_forest = RandomForestClassifier(n_estimators=140,min_samples_split=10, min_samples_leaf=2, max_features='sqrt', max_depth=40)\n",
    "model_forest.fit(x_train,y_train)\n",
    "prediction = model_forest.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.76      0.65      1862\n",
      "           0       0.89      0.59      0.71      2367\n",
      "           1       0.75      0.79      0.77      3172\n",
      "\n",
      "    accuracy                           0.72      7401\n",
      "   macro avg       0.74      0.71      0.71      7401\n",
      "weighted avg       0.75      0.72      0.72      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_nb = MultinomialNB(alpha=1.0, fit_prior=False)\n",
    "model_nb = model_nb.fit(x_train,y_train)\n",
    "prediction_nb = model_nb.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediction_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.81      0.82      1862\n",
      "           0       0.94      0.97      0.96      2367\n",
      "           1       0.91      0.89      0.90      3172\n",
      "\n",
      "    accuracy                           0.90      7401\n",
      "   macro avg       0.89      0.89      0.89      7401\n",
      "weighted avg       0.90      0.90      0.90      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(kernel='linear',degree=1, C=1)\n",
    "model_svc = model_svc.fit(x_train,y_train)\n",
    "prediction_svc = model_svc.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(df_tfidf, df['senti_textblob'],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.5s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.5s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.5s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.5s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   6.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   5.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=110; total time=   5.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.2s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   5.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   5.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=90; total time=   6.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=140; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=180; total time=   3.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   7.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=170; total time=   2.4s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   5.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   4.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=70; total time=   5.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.1s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=90; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=15, n_estimators=140; total time=   1.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   6.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   5.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   6.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   5.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   6.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   6.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   6.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   6.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   5.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=110; total time=   5.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 40}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for RF(with TFIDF)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RF_RandomGrid = RandomizedSearchCV(estimator = model_forest, param_distributions = param_grid_forest, cv = 10, verbose=2, n_jobs = 4)\n",
    "RF_RandomGrid.fit(x_train_2, y_train_2)\n",
    "RF_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.5, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.1, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': False, 'alpha': 0.5}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_RandomGrid = RandomizedSearchCV(estimator = model_nb, param_distributions = param_grid_nb, cv = 10, verbose=2, n_jobs = 4)\n",
    "NB_RandomGrid.fit(x_train_2, y_train_2)\n",
    "NB_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.5min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.7min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.8min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.8min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.6min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.5min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.6min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.9min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.3min\n",
      "[CV] END .......................C=100, degree=1, kernel=poly; total time= 3.2min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 3.3min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 3.3min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 2.8min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 3.1min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 3.0min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 3.0min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 2.7min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 3.2min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 2.9min\n",
      "[CV] END ......................C=1000, degree=1, kernel=poly; total time= 2.9min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.9min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 5.0min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.9min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.8min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.9min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.8min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.9min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.9min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 3.1min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 3.1min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.8min\n",
      "[CV] END .......................C=1000, degree=3, kernel=rbf; total time= 4.9min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 2.7min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 3.1min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 3.0min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 3.0min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 3.0min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 2.6min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 2.8min\n",
      "[CV] END ....................C=1000, degree=2, kernel=linear; total time= 2.8min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.1min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.1min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.2min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.1min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.0min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.1min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.5min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.5min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.4min\n",
      "[CV] END .....................C=100, degree=3, kernel=linear; total time= 3.5min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.4min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.5min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.4min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.5min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.3min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.4min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.3min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.4min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.4min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.4min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END .......................C=100, degree=2, kernel=poly; total time= 6.6min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.9min\n",
      "[CV] END .....................C=10, degree=2, kernel=sigmoid; total time= 1.8min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 2.5min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 1.8min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 1.8min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 1.9min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END ...................C=1000, degree=3, kernel=sigmoid; total time= 1.6min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 4.1min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 3.7min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 3.8min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 3.5min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 3.7min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 3.7min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 4.0min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 4.1min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 3.9min\n",
      "[CV] END ......................C=10, degree=3, kernel=linear; total time= 4.0min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 7.9min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 8.4min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 8.2min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 8.4min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 8.3min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 8.3min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 8.5min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 8.2min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 7.5min\n",
      "[CV] END .......................C=100, degree=4, kernel=poly; total time= 7.3min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kernel': 'linear', 'degree': 3, 'C': 10}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_RandomGrid = RandomizedSearchCV(estimator = model_svc, param_distributions = param_grid_svc, cv = 10, verbose=2, n_jobs = 4)\n",
    "SVC_RandomGrid.fit(x_train_2, y_train_2)\n",
    "SVC_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.16      0.27      1799\n",
      "           0       0.80      0.72      0.76      2344\n",
      "           1       0.62      0.95      0.75      3258\n",
      "\n",
      "    accuracy                           0.68      7401\n",
      "   macro avg       0.79      0.61      0.59      7401\n",
      "weighted avg       0.76      0.68      0.63      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model establishment and results(TF-IDF)\n",
    "#Random Forest\n",
    "model_forest = RandomForestClassifier(n_estimators=120,min_samples_split=5, min_samples_leaf=2, max_features='sqrt', max_depth=40)\n",
    "#model_forest = RandomForestClassifier()\n",
    "model_forest.fit(x_train_2,y_train_2)\n",
    "prediction = model_forest.predict(x_test_2)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test_2, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.69      0.64      1799\n",
      "           0       0.88      0.58      0.70      2344\n",
      "           1       0.71      0.83      0.77      3258\n",
      "\n",
      "    accuracy                           0.72      7401\n",
      "   macro avg       0.73      0.70      0.70      7401\n",
      "weighted avg       0.74      0.72      0.72      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_nb = MultinomialNB(alpha=0.5, fit_prior=False)\n",
    "model_nb = model_nb.fit(x_train_2,y_train_2)\n",
    "prediction_nb = model_nb.predict(x_test_2)\n",
    "\n",
    "print(classification_report(y_test_2, prediction_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.79      0.79      1799\n",
      "           0       0.91      0.96      0.94      2344\n",
      "           1       0.90      0.87      0.88      3258\n",
      "\n",
      "    accuracy                           0.88      7401\n",
      "   macro avg       0.87      0.87      0.87      7401\n",
      "weighted avg       0.88      0.88      0.88      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(C=10, kernel='linear',degree=3)\n",
    "model_svc = model_svc.fit(x_train_2,y_train_2)\n",
    "prediction_svc = model_svc.predict(x_test_2)\n",
    "\n",
    "print(classification_report(y_test_2, prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(tweet_vec_df, df['senti_textblob'],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_forest = RandomForestClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=10; total time=   1.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=160; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.8s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  16.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.7s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  16.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.8s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.8s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=40; total time=  15.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.8s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  30.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.8s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=60; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=10, min_samples_split=15, n_estimators=130; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=15, n_estimators=20; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  29.8s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=  28.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  16.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.5s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  15.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=180; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=5, min_samples_split=15, n_estimators=40; total time=  14.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 180,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 40}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best parameters for RF(with Word2vec)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RF_RandomGrid = RandomizedSearchCV(estimator = model_forest, param_distributions = param_grid_forest, cv = 10, verbose=2, n_jobs = 4)\n",
    "RF_RandomGrid.fit(x_train_3, y_train_3)\n",
    "RF_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.1min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 2.9min\n",
      "[CV] END ........................C=10, degree=4, kernel=poly; total time= 3.3min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END .....................C=10, degree=4, kernel=sigmoid; total time= 2.2min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.2min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.2min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.2min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.2min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.3min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.2min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.2min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.2min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.5min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.5min\n",
      "[CV] END ..........................C=1, degree=3, kernel=rbf; total time= 3.5min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.5min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .....................C=10, degree=1, kernel=sigmoid; total time= 2.6min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.2min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.3min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.5min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.1min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.6min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.5min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.6min\n",
      "[CV] END .......................C=100, degree=3, kernel=poly; total time= 8.6min\n"
     ]
    }
   ],
   "source": [
    "SVC_RandomGrid = RandomizedSearchCV(estimator = model_svc, param_distributions = param_grid_svc, cv = 10, verbose=2, n_jobs = 4)\n",
    "SVC_RandomGrid.fit(x_train_3, y_train_3)\n",
    "SVC_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=5.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END .........................alpha=0.01, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=0.1, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=10.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=0.01, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=5.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ........................alpha=10.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END .........................alpha=1.0, fit_prior=False; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=0.5, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n",
      "[CV] END ..........................alpha=1.0, fit_prior=True; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_prior': True, 'alpha': 0.01}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() #handle negative\n",
    "x_train_3a = scaler.fit_transform(x_train_3)\n",
    "x_test_3a = scaler.fit_transform(x_test_3)\n",
    "NB_RandomGrid = RandomizedSearchCV(estimator = model_nb, param_distributions = param_grid_nb, cv = 10, verbose=2, n_jobs = 4)\n",
    "NB_RandomGrid.fit(x_train_3a, y_train_3)\n",
    "NB_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.39      0.11      0.17       149\n",
      "           0       0.56      0.52      0.54       185\n",
      "           1       0.52      0.75      0.61       266\n",
      "\n",
      "    accuracy                           0.52       600\n",
      "   macro avg       0.49      0.46      0.44       600\n",
      "weighted avg       0.50      0.52      0.48       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model establishment and results(Word2Vec)\n",
    "#Random Forest\n",
    "model_forest = RandomForestClassifier(n_estimators=160,min_samples_split=15, min_samples_leaf=2, max_features='log2', max_depth=30)\n",
    "model_forest.fit(x_train_3,y_train_3)\n",
    "prediction = model_forest.predict(x_test_3)\n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "print(classification_report(y_test_3, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.11      0.18       149\n",
      "           0       0.57      0.54      0.55       185\n",
      "           1       0.56      0.81      0.66       266\n",
      "\n",
      "    accuracy                           0.55       600\n",
      "   macro avg       0.51      0.49      0.46       600\n",
      "weighted avg       0.53      0.55      0.51       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# x_train_3 = scaler.fit_transform(x_train_3)\n",
    "# x_test_3 = scaler.fit_transform(x_test_3)\n",
    "\n",
    "model_nb = MultinomialNB(alpha=10, fit_prior=False)\n",
    "model_nb = model_nb.fit(x_train_3a,y_train_3)\n",
    "prediction_nb = model_nb.predict(x_test_3a)\n",
    "\n",
    "print(classification_report(y_test_3, prediction_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.28      0.30      0.29       149\n",
      "           0       0.31      0.64      0.42       185\n",
      "           1       0.49      0.12      0.20       266\n",
      "\n",
      "    accuracy                           0.33       600\n",
      "   macro avg       0.36      0.35      0.30       600\n",
      "weighted avg       0.38      0.33      0.29       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(C=1000, kernel='linear', degree=2)\n",
    "model_svc = model_svc.fit(x_train_3,y_train_3)\n",
    "prediction_svc = model_svc.predict(x_test_3)\n",
    "\n",
    "print(classification_report(y_test_3, prediction_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
